{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPi3XuqTUT02arV6fJ8zNqH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maahir-garg/llama2-finetuning/blob/main/creating_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets transformers sentence_transformers faiss-gpu"
      ],
      "metadata": {
        "id": "VPzx7Nqpeymj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('huggingface')"
      ],
      "metadata": {
        "id": "GuBk0QqpoTeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('garage-bAInd/Open-Platypus')"
      ],
      "metadata": {
        "id": "uSwRtIU2Octj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "xJJoQIBIOq4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('NousResearch/Llama-2-7b-hf')"
      ],
      "metadata": {
        "id": "H3amqHlKOze2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_token_counts = [len(tokenizer.tokenize(example[\"instruction\"])) for example in dataset[\"train\"]]\n",
        "output_token_counts = [len(tokenizer.tokenize(example[\"output\"])) for example in dataset[\"train\"]]\n",
        "combined_token_counts = [instruction + output for instruction, output in zip(instruction_token_counts, output_token_counts)]\n",
        "combined_token_counts"
      ],
      "metadata": {
        "id": "zcbj74wBPhpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_distribution(token_counts, title):\n",
        "  sns.set_style(\"whitegrid\")\n",
        "  plt.figure(figsize=(15, 6))\n",
        "  plt.hist(token_counts, bins=50, color=\"#3498db\", edgecolor=\"black\")\n",
        "  plt.title(title, fontsize=16)\n",
        "  plt.xlabel(\"Number of tokens\", fontsize=14)\n",
        "  plt.ylabel(\"Number of examples\", fontsize=14)\n",
        "  plt.xticks(fontsize=12)\n",
        "  plt.yticks(fontsize=12)\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_distribution(instruction_token_counts, \"Distribution of token counts for instruction\")\n",
        "plot_distribution(output_token_counts, \"Distribution of token counts for output\")\n",
        "plot_distribution(combined_token_counts, \"Distribution of token counts for combined\")"
      ],
      "metadata": {
        "id": "OoAaqn_dRnVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_indices = [i for i, count in enumerate(combined_token_counts) if count <= 2048]\n",
        "print(len(dataset[\"train\"]) - len(valid_indices))\n",
        "\n",
        "dataset[\"train\"] = dataset[\"train\"].select(valid_indices)\n",
        "\n",
        "token_counts = [combined_token_counts[i] for i in valid_indices]\n",
        "\n",
        "plot_distribution(token_counts, \"Combined distribution after filtering\")"
      ],
      "metadata": {
        "id": "PT0TdzlnS4P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from datasets import Dataset, DatasetDict\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def deduplicate_dataset(dataset: Dataset, model: str, threshold: float):\n",
        "  sentence_model = SentenceTransformer(model)\n",
        "  outputs = [example[\"output\"] for example in dataset[\"train\"]]\n",
        "\n",
        "  print(\"Converting text to embeddings..\")\n",
        "  embeddings = sentence_model.encode(outputs, show_progress_bar=True)\n",
        "  dimension = embeddings.shape[1]\n",
        "  index = faiss.IndexFlatIP(dimension)\n",
        "  normalised_embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "  index.add(normalised_embeddings)\n",
        "\n",
        "  print(\"Filtering out near duplicates..\")\n",
        "  D, I = index.search(normalised_embeddings, k=2)\n",
        "  to_keep = []\n",
        "  for i in tqdm(range(len(embeddings)), desc=\"Filtering\"):\n",
        "    if D[i, 1] < threshold:\n",
        "      to_keep.append(i)\n",
        "  dataset = dataset[\"train\"].select(to_keep)\n",
        "  return DatasetDict({\"train\": dataset})\n",
        "\n",
        "deduped_dataset = deduplicate_dataset(dataset, \"thenlper/gte-large\", 0.95)"
      ],
      "metadata": {
        "id": "Co-GFJjwUCHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset[\"train\"]))\n",
        "print(len(deduped_dataset[\"train\"]))\n",
        "print(len(deduped_dataset[\"train\"]) - len(dataset[\"train\"]))"
      ],
      "metadata": {
        "id": "xfVXnkgscx-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_k_rows(dataset, token_counts, k):\n",
        "  sorted_indices = sorted(range(len(token_counts)), key= lambda i: token_counts[i], reverse=True)\n",
        "  top_k_indices = sorted_indices[:k]\n",
        "\n",
        "  top_k_data = {\n",
        "      \"instruction\": [dataset[\"train\"][i][\"instruction\"] for i in top_k_indices],\n",
        "      \"output\": [dataset[\"train\"][i][\"output\"] for i in top_k_indices]\n",
        "  }\n",
        "\n",
        "  return Dataset.from_dict(top_k_data)\n",
        "\n",
        "k = 1000\n",
        "instruction_token_counts = [len(tokenizer.tokenize(example[\"instruction\"])) for example in dataset[\"train\"]]\n",
        "output_token_counts = [len(tokenizer.tokenize(example[\"output\"])) for example in dataset[\"train\"]]\n",
        "combined_token_counts = [instruction + output for instruction, output in zip(instruction_token_counts, output_token_counts)]\n",
        "\n",
        "top_k_dataset = get_top_k_rows(dataset, combined_token_counts, k)\n",
        "dataset = DatasetDict({\"train\": top_k_dataset})\n"
      ],
      "metadata": {
        "id": "BsDmAY1QaCw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instruction_token_counts = [len(tokenizer.tokenize(example[\"instruction\"])) for example in dataset[\"train\"]]\n",
        "output_token_counts = [len(tokenizer.tokenize(example[\"output\"])) for example in dataset[\"train\"]]\n",
        "combined_token_counts = [instruction + output for instruction, output in zip(instruction_token_counts, output_token_counts)]\n",
        "\n",
        "plot_distribution(instruction_token_counts, \"Distribution of token counts for instruction\")\n",
        "plot_distribution(output_token_counts, \"Distribution of token counts for output\")\n",
        "plot_distribution(combined_token_counts, \"Distribution of token counts for combined\")"
      ],
      "metadata": {
        "id": "UuR6Raufbw1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "5zIHgyJyb1Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_template(example):\n",
        "    example[\"instruction\"] = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(chat_template)"
      ],
      "metadata": {
        "id": "-D2HE3xNcKJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"].to_pandas()"
      ],
      "metadata": {
        "id": "8QMy8DCIg89w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"refined-platypus\", token=hf_token)"
      ],
      "metadata": {
        "id": "SsAGcLDcmB6e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}